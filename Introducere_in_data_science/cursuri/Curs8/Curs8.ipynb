{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curs 8. Lucrul cu date de tip text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: prezentarea si codul sun conform [Introduction to Machine Learning with Python: A Guide for Data Scientists](https://www.bookdepository.com/Introduction-Machine-Learning-with-Python-Andreas-C-Mueller-Sarah-Guido/9781449369415)\n",
    "\n",
    "O clasa aparte de probleme este cea data de procesarea de informatie de tip text. Exemple de probleme:\n",
    "* clasificarea mailului ca fiind de tip spam sau legitim\n",
    "* analiza sentimentelor pe baza elementelor scrise\n",
    "* regasirea de texte similare\n",
    "\n",
    "Exista urmatoarele 4 categorii de date text:\n",
    "1. date categoriale\n",
    "1. text liber reductibil la date categoriale\n",
    "1. text structurat\n",
    "1. text liber\n",
    "\n",
    "Datele categoriale rpovin dinstr-o lista predefinita (ex: genul unei persoane, culoarea unei masini). Totusi, in functie de modul de prelaure a valorii de la utilizator, este inca posibil sa apara erori de scriere: blak in loc de black, sau particularitati de scriere: gray/grey, neighbor/neighbour. Se impune deci o determinare a formelor corecte, de exemplu prin distanta Levenshtein sau algoritmi de spell-checking.\n",
    "\n",
    "Textul liber reductibil la date categoriale se refera la exprimari care pot fi reduse la date text categoriale: culoara ierbii -> verde, sau definirea unor categorii de ocupatii iar la final \"Altele\"; un exemplu bun este [Color Survey Results](https://blog.xkcd.com/2010/05/03/color-survey-results/).\n",
    "\n",
    "Textul structurat poate fi exemplificat prin documente XML sau fisiere JSON, care respecta o anumita sintaxa si eventual o schema. Procesarea lor este un subiect separat. \n",
    "\n",
    "Ultima categorie este data de fisiere email, carti, tweets, scrisori de intentie etc. Procesarea lor intra in zona Natural Language Processing (NLP) si information retrieval (IR). Un exemplu este  determinarea plagiatelor, prin care se detecteaza preluari masive din alte surse text, sau atribuirea autorului - pentru o bucata de text se cere determinarea autorului cel mai probabil. \n",
    "\n",
    "Sursele de date sunt diverse: \n",
    "* paginile Wikipedia\n",
    "* cartile din [proiectul Gutenberg](http://www.gutenberg.org/), la ora actuaal 56000 ebooks\n",
    "* [mesaje newgroups](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html)\n",
    "* [ziare](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplu de aplicatie: analiza sentimentor din recenzii de filme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista un set de recenzii de filme facute de catre utilizator, disponibil [aici](http://ai.stanford.edu/~amaas/data/sentiment/). Pentru fiecare recenzie exista o valoare numerica intre 1 si 10, reprezentand o indicatie: daca este un review pozitiv sau negativ. Setul de date este impartit in subset de antrenare si de testare, iar pentru fiecare subset se gasesc doua directoare, respectiv pentru aprecieri pozitive si negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is data\n",
      " Volume Serial Number is 503D-AB7C\n",
      "\n",
      " Directory of D:\\work\\school\\cursuri\\cursuri_github\\cursuri\\Introducere_in_data_science\\cursuri\\Curs8\\data\\aclImdb\n",
      "\n",
      "06/26/2011  04:08 AM    <DIR>          .\n",
      "06/26/2011  04:08 AM    <DIR>          ..\n",
      "04/12/2011  08:14 PM           845,980 imdb.vocab\n",
      "06/12/2011  01:54 AM           903,029 imdbEr.txt\n",
      "06/26/2011  03:18 AM             4,037 README\n",
      "04/12/2011  08:22 PM    <DIR>          test\n",
      "06/26/2011  04:09 AM    <DIR>          train\n",
      "               3 File(s)      1,753,046 bytes\n",
      "               4 Dir(s)  1,542,084,423,680 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir \"data/aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/aclImdb/'\n",
    "path_train = path + 'train/'\n",
    "path_test = path + 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = load_files(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are texts organized:  <class 'list'>\n",
      "How many texts in train subset 75000\n",
      "The first text:  b'Full of (then) unknown actors TSF is a great big cuddly romp of a film.<br /><br />The idea of a bunch of bored teenagers ripping off the local sink factory is odd enough, but add in the black humour that Forsyth & Co are so good at and your in for a real treat.<br /><br />The comatose van driver by itself worth seeing, and the canal side chase is just too real to be anything but funny.<br /><br />And for anyone who lived in Glasgow it\\'s a great \"Oh I know where that is\" film.'\n",
      "Associated review: 2\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print('How are texts organized: ', type(text_train))\n",
    "print('How many texts in train subset', len(text_train))\n",
    "print('The first text: ', text_train[0])\n",
    "print('Associated review:', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_test = load_files(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are texts organized:  <class 'list'>\n",
      "How many texts in train subset 25000\n",
      "The first text:  b\"Don't hate Heather Graham because she's beautiful, hate her because she's fun to watch in this movie. Like the hip clothing and funky surroundings, the actors in this flick work well together. Casey Affleck is hysterical and Heather Graham literally lights up the screen. The minor characters - Goran Visnjic {sigh} and Patricia Velazquez are as TALENTED as they are gorgeous. Congratulations Miramax & Director Lisa Krueger!\"\n",
      "Associated review: 1\n"
     ]
    }
   ],
   "source": [
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print('How are texts organized: ', type(text_test))\n",
    "print('How many texts in train subset', len(text_test))\n",
    "print('The first text: ', text_test[0])\n",
    "print('Associated review:', y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se remarca existenta elementului html `<br />` ce poate fi inlaturat, fara a afecta continutul mesajului:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [text.replace(b'<br />', b' ') for text in text_train]\n",
    "text_test = [text.replace(b'<br />', b' ') for text in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numarul de elemente din fiecare clasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in train set:  [0 1 2]\n",
      "Classes in test set:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Classes in train set: ', np.unique(y_train))\n",
    "print('Classes in test set: ', np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out items of class \"2\" in train set\n",
    "text_train = [text_train[i] for i in range(len(y_train)) if y_train[i] < 2 ]\n",
    "y_train = [y_train[i] for i in range(len(y_train)) if y_train[i] < 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class in training set: [12500 12500]\n",
      "Samples per class in test set: [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print('Samples per class in training set: {0}'.format(np.bincount(y_train)))\n",
    "print('Samples per class in test set: {0}'.format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentarea textului sub forma bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru un text, reprezenatrea bag of word se obtine astfel: se pleaca de la un vocabular  = multimea tuturor cuvintelor care pot aparea in texte - si se contorizeaza pentru fiecare cuvant din dictionar de cat ori apare in text. Daca vocabularul are $N$ cuvinte distincte, atunci pentru fiecare document rezulta un vector de $N$ componente. Majoritatea componentelor din vectorul asociat unui document va fi zeri, deci avem de-a face cu vectori rari. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasii pentru obtinerea unei reprezentari bag of words sunt:\n",
    "1. despartirea in cuvuinte (tokenizing)\n",
    "1. construirea dictionarului\n",
    "1. construirea documentului bag of word pentru document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplu de obtinere de bag of words pe un text simplu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words =[\n",
    "    \"The fool doth think he is wise,\",\n",
    "    \"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasa `CountVectorizer` din `sklearn.feature_extraction.text` serveste la selectarea cuvintelor din text si calculul frecventei de aparitie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtinerea unui vector bag of words se obtine prin aplicarea metodei `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = vect.transform(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprezentarea ca vectori rari:   (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "Reprezentarea ca vectori:\n",
      " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print('Reprezentarea ca vectori rari:', bow)\n",
    "print('Reprezentarea ca vectori:\\n', bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformarea celor doua subseturi de date in BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(text_train)\n",
    "print(repr(X_train))\n",
    "X_test = vect.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiunea vocabularului: 74849\n"
     ]
    }
   ],
   "source": [
    "#obtinerea vocabularului\n",
    "feature_names = vect.get_feature_names()\n",
    "print('Dimensiunea vocabularului:', len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['är', 'ääliöt', 'äänekoski', 'åge', 'åmål', 'æsthetic', 'écran', 'élan', 'émigré', 'émigrés', 'était', 'état', 'étc', 'évery', 'êxtase', 'ís', 'ísnt', 'østbye', 'über', 'üvegtigris']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draper', 'draperies', 'drapery', 'drapes', 'draskovic', 'drastic', 'drastically', 'drat', 'dratch', 'dratic', 'dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[20000:20020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antrenarea si evaluarea unui model logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea medie de clasificare: 0.8819600000000001\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv = 5, n_jobs=4)\n",
    "print('Acuratetea medie de clasificare: {0}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicare de cross validation pentru parametrul de regularizare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista un parametru de regularizare care determina penalizarea $L_2$ a ponderilor modelului. Efectum cautarea valorii potrivite a acestui hiperparametru, al carui nume de argument este `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.88812\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87892\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificarea parametrilor pentru obtinerea BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putem cere retinerea doar a acelor cuvinte care apar intr-un numar minim de documente, de ex 5. In acest fel speram ca se elimina cuvintele care nu sunt larg partajate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular:  27271\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5)\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "print('Dimensiune vocabular: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.8874\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87784\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminare cuvinte neinformative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista cuvinte care nu poarta multa informatie: prepozitii, conjunctii, sau cuvinte care se repeta des in toate documentele. Exista doua posibiliatti de eliminare a acestora:\n",
    "1. Se foloseste o lista predefiita de stopwords, specifica limbii respective\n",
    "1. Se estimeaza valoarea informationala a cuvintelor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminarea de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru limba engleza sunt definite stopwords in sklearn.feature_extraction.text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(len(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itself', 'nevertheless', 'five', 'others', 'sometimes', 'also', 'thin', 'what', 'than', 'sometime', 'formerly', 'themselves', 'thereafter', 'never', 'still', 'whenever', 'because', 'anywhere', 'me', 'please']\n"
     ]
    }
   ],
   "source": [
    "print(list(ENGLISH_STOP_WORDS)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular:  26966\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words='english')\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "print('Dimensiune vocabular: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.88364\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87252\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculul valorii tf-df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency - inverse document frequency da pondere unui cuvant care apare frecvent intr-un document, dar nu in multe documente - deci apre putere descriptiva pentru documentul in care apare frecvent. Pentru un document $d$ si un cuvant $w$, valoarea tfidf se calculeaza ca:\n",
    "$$\n",
    "tfidf(w, d) = tf(w, d) \\cdot \\log\\left( \\frac{N+1}{N_w+1} \\right) + 1\n",
    "$$\n",
    "unde: $tf(w, d)$ este numarul de aparitii ale lui $w$ in docuemntul $d$, $N$ e numarul total de documente din corpus, $N_w$ este numarul de documente din corpus care contin cuvantul $w$. Pentru fiecare document $d$ se calculeaza un astfel e de vector, care in final este normalizat in norma $L_2$ la 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.89416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None), LogisticRegression())\n",
    "param_grid = {'logisticregression__C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(text_train, y_train)\n",
    "print('Best cross validation score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88504000000000005"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizarea de n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW pierde succesiunea cuvintelor. Se pot considera toate succesiunile de n cuvinte - n-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The fool doth think he is wise,', 'but the wise man knows himself to be a fool']\n"
     ]
    }
   ],
   "source": [
    "print(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be fool',\n",
       " 'but the',\n",
       " 'doth think',\n",
       " 'fool doth',\n",
       " 'he is',\n",
       " 'himself to',\n",
       " 'is wise',\n",
       " 'knows himself',\n",
       " 'man knows',\n",
       " 'the fool',\n",
       " 'the wise',\n",
       " 'think he',\n",
       " 'to be',\n",
       " 'wise man']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(text_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
