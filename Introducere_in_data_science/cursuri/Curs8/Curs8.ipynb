{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curs 8. Lucrul cu date de tip text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: prezentarea si codul sunt conform [Introduction to Machine Learning with Python: A Guide for Data Scientists](https://www.bookdepository.com/Introduction-Machine-Learning-with-Python-Andreas-C-Mueller-Sarah-Guido/9781449369415)\n",
    "\n",
    "O clasa aparte de probleme este cea data de procesarea de informatie de tip text. Exemple de probleme:\n",
    "* clasificarea mailului ca fiind de tip spam sau legitim\n",
    "* analiza sentimentelor pe baza elementelor scrise\n",
    "* regasirea de texte similare\n",
    "\n",
    "Exista urmatoarele 4 categorii de date text:\n",
    "1. date categoriale\n",
    "1. text liber reductibil la date categoriale\n",
    "1. text structurat\n",
    "1. text liber\n",
    "\n",
    "Datele categoriale provin dintr-o lista predefinita (ex: genul unei persoane, culoarea unei masini):\n",
    "* \"red\", \"green\", \"blue\", \"yellow\", \"black\", \"white\", \"purple\", \"pink\"; \"multicolo(u)red\", \"other\"\n",
    "* \"male\", \"female\", \"not declared\". \n",
    "\n",
    "Totusi, in functie de modul de prelaure a valorii de la utilizator, este inca posibil sa apara erori de scriere: blak in loc de black, sau particularitati de scriere: gray/grey, neighbor/neighbour. Se impune deci o determinare a formelor corecte, de exemplu prin [distanta Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance) sau [algoritmi de spell-checking](https://norvig.com/spell-correct.html) - atentie la dialectul ales. In alte cazuri este nevoie de reducerea dictionarului, pentru notiuni care sunt foarte similare sau identice: \n",
    "* \"culoarea ierbii\" -> \"verde\", \n",
    "* definirea unor categorii de ocupatii iar la final \"Altele\"\n",
    "* [Colours](http://www.thedoghousediaries.com/1406)\n",
    "\n",
    "In alte cazuri, exprimari libere (texte de dimensiuni medii) pot fi reduse la categorii cunoscute. Pentru simplificarea procesarii, se recomanda evitarea de introducere de text liber si sa se permita alegerea dintr-o multime predefinita de valori:\n",
    "\n",
    "<img src=\"./images/dropdown.gif\" alt=\"Dropdown\" width=\"200px\"/>\n",
    "\n",
    "Textul structurat poate fi exemplificat prin documente XML sau fisiere JSON, care respecta o anumita sintaxa si eventual o schema. De exemplu, datele de contact au o structura (adresa fizica, persoana de contact, telefon), interpretarea lor facandu-se cu cunostinte de domeniu. Procesarea lor este un subiect separat. \n",
    "\n",
    "Ultima categorie este data de documente de intindere mai mare: email, carti, tweets, scrisori de intentie etc. Procesarea lor intra in zona Natural Language Processing (NLP) si information retrieval (IR). Un exemplu este  determinarea plagiatelor, prin care se detecteaza preluari masive din alte surse text, sau atribuirea autorului - pentru o bucata de text se cere determinarea autorului cel mai probabil. \n",
    "\n",
    "Sursele de date sunt diverse: \n",
    "* pagini Wikipedia\n",
    "* cartile din [proiectul Gutenberg](http://www.gutenberg.org/), la ora (aprilie 2019) peste 58000 de ebooks\n",
    "* [mesaje newgroups](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html)\n",
    "* [ziare](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplu de aplicatie: analiza sentimentor din recenzii de filme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista un set de recenzii de filme facute de catre utilizator, disponibil [aici](http://ai.stanford.edu/~amaas/data/sentiment/). Pentru fiecare recenzie exista o valoare numerica intre 1 si 10, reprezentand o indicatie: daca este un review pozitiv sau negativ. Setul de date este impartit in subset de antrenare si de testare, iar pentru fiecare subset se gasesc doua directoare, respectiv pentru aprecieri pozitive (rating > 5) si negative (rating <= 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:22:20.793645Z",
     "start_time": "2019-04-11T05:22:20.643051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is data\n",
      " Volume Serial Number is 503D-AB7C\n",
      "\n",
      " Directory of d:\\work\\school\\cursuri\\Introducere_In_Data_Science\\cursuri\\Curs8\\data\\aclImdb\n",
      "\n",
      "05/25/2018  07:09 AM    <DIR>          .\n",
      "05/25/2018  07:09 AM    <DIR>          ..\n",
      "04/12/2011  08:14 PM           845,980 imdb.vocab\n",
      "06/12/2011  01:54 AM           903,029 imdbEr.txt\n",
      "06/26/2011  03:18 AM             4,037 README\n",
      "05/25/2018  07:03 AM    <DIR>          test\n",
      "05/25/2018  07:09 AM    <DIR>          train\n",
      "               3 File(s)      1,753,046 bytes\n",
      "               4 Dir(s)  718,327,115,776 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir \"data/aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:22:24.657069Z",
     "start_time": "2019-04-11T05:22:20.796638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder PATH listing for volume data\n",
      "Volume serial number is 503D-AB7C\n",
      "D:\\WORK\\SCHOOL\\CURSURI\\INTRODUCERE_IN_DATA_SCIENCE\\CURSURI\\CURS8\\DATA\\ACLIMDB\n",
      "+---test\n",
      "|   +---neg\n",
      "|   \\---pos\n",
      "\\---train\n",
      "    +---neg\n",
      "    +---pos\n",
      "    \\---unsup\n"
     ]
    }
   ],
   "source": [
    "!tree \"data/aclImdb\" /A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:22:24.669035Z",
     "start_time": "2019-04-11T05:22:24.661057Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './data/aclImdb/'\n",
    "path_train = path + 'train/'\n",
    "path_test = path + 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:22:31.380536Z",
     "start_time": "2019-04-11T05:22:24.672028Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:38:42.459560Z",
     "start_time": "2019-04-11T05:22:31.382438Z"
    }
   },
   "outputs": [],
   "source": [
    "####### !!!16 mins running time!!!\n",
    "reviews_train = load_files(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:38:42.472395Z",
     "start_time": "2019-04-11T05:38:42.462454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are texts organized:  <class 'list'>\n",
      "How many texts in train subset 75000\n",
      "The first text:  b'Full of (then) unknown actors TSF is a great big cuddly romp of a film.<br /><br />The idea of a bunch of bored teenagers ripping off the local sink factory is odd enough, but add in the black humour that Forsyth & Co are so good at and your in for a real treat.<br /><br />The comatose van driver by itself worth seeing, and the canal side chase is just too real to be anything but funny.<br /><br />And for anyone who lived in Glasgow it\\'s a great \"Oh I know where that is\" film.'\n",
      "Associated review: 2\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print('How are texts organized: ', type(text_train))\n",
    "print('How many texts in train subset', len(text_train))\n",
    "print('The first text: ', text_train[0])\n",
    "print('Associated review:', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.277915Z",
     "start_time": "2019-04-11T05:38:42.475390Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_test = load_files(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.291799Z",
     "start_time": "2019-04-11T05:41:30.280830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are texts organized:  <class 'list'>\n",
      "How many texts in train subset 25000\n",
      "The first text:  b'I HATE THIS MOVIE!!!!!! I have never seen such utter, complete trash in my life!!! I live in France so it turned out that I was in the front line to watch this awful movie. At first, it seemed cool, kind of like something about a cursed forest that chomps people. Unfortunately, it turned out it was something QUITE different: a good start with a girl that meets a guy and all that whatnot, then the girl gets threatening messages in the form of ravens shut up in her bathroom closet(ludicrous), from that bit and on, the movie starts to slide downhill very quickly with a lot of desperate thrashing in the process. The movie ends with sacrificial druids galore and ancient ugly, stinky creatures coming back from the past to kill a few people. Many questions were rushing around my head by then: why the heck did they bring back that scummy monster? Do druids look like maniacs dressed in bedsheets? Why did they even bother making this movie?? The \"climax\" of the movie was so goofy I laughed all the way through it: the \"awful\" stinky monster does battle with the two young women(who appear to be expert kung fu masters) and the professor gets sliced in two or something. What surprised me was that the monster was so slow and ungainly in battle, wasn\\'t it supposed to be a god of war or something? Anyway, the movie in it\\'s death throws was a pitiful sight. A brief condensation of the contents of this movie: Kung fu mayhem+druid stones+mysterious murders \"\\xc3\\xa0 la thriller\"+ancient prophecies+shabby ravens+old clumsy boneless war god+nutty professor=complete and utter, diseased, boneless, worm eaten, GODFORSAKEN, GODDAM, RECYCLED, FAKE, WANNABE, LUDICROUS SH*T!!!!!!!!! Things I learned from this movie: -Ancient war gods are lousy at kung fu. -All young women who study archeology at university in France are kung fu experts. -Professors are so resistant they can survive being sliced in two by a saw-mass-whatchamacallit without any injuries.'\n",
      "Associated review: 0\n"
     ]
    }
   ],
   "source": [
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print('How are texts organized: ', type(text_test))\n",
    "print('How many texts in train subset', len(text_test))\n",
    "print('The first text: ', text_test[10])\n",
    "print('Associated review:', y_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se remarca existenta elementului html `<br />` ce poate fi inlaturat, fara a afecta continutul mesajului:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.643920Z",
     "start_time": "2019-04-11T05:41:30.295788Z"
    }
   },
   "outputs": [],
   "source": [
    "text_train = [text.replace(b'<br />', b' ') for text in text_train]\n",
    "text_test = [text.replace(b'<br />', b' ') for text in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numarul de elemente din fiecare clasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.657820Z",
     "start_time": "2019-04-11T05:41:30.644854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in train set:  [0 1 2]\n",
      "Classes in test set:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Classes in train set: ', np.unique(y_train))\n",
    "print('Classes in test set: ', np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.745676Z",
     "start_time": "2019-04-11T05:41:30.659817Z"
    }
   },
   "outputs": [],
   "source": [
    "#Filtering out items of class \"2\" in train set\n",
    "text_train = [text_train[i] for i in range(len(y_train)) if y_train[i] < 2 ]\n",
    "y_train = [y_train[i] for i in range(len(y_train)) if y_train[i] < 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.757555Z",
     "start_time": "2019-04-11T05:41:30.747580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class in training set: [12500 12500]\n",
      "Samples per class in test set: [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print('Samples per class in training set: {0}'.format(np.bincount(y_train)))\n",
    "print('Samples per class in test set: {0}'.format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentarea textului sub forma bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru un text, reprezenatrea bag of word se obtine astfel: se pleaca de la un vocabular  = multimea tuturor cuvintelor care pot aparea in texte - si se contorizeaza pentru fiecare cuvant din dictionar de cat ori apare in text. Daca vocabularul are $k$ cuvinte distincte, atunci pentru fiecare document rezulta un vector de $k$ componente. Majoritatea componentelor din vectorul asociat unui document va fi zero, deci avem de-a face cu vectori rari. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasii pentru obtinerea unei reprezentari bag of words sunt:\n",
    "1. despartirea in cuvinte (tokenizing)\n",
    "1. construirea dictionarului\n",
    "1. construirea documentului bag of word pentru document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bow_processing.png' width='600px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplu de obtinere de bag of words pe un text simplu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.765533Z",
     "start_time": "2019-04-11T05:41:30.760545Z"
    }
   },
   "outputs": [],
   "source": [
    "bards_words =[\n",
    "    \"The fool doth think he is wise,\",\n",
    "    \"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasa `CountVectorizer` din `sklearn.feature_extraction.text` serveste la selectarea cuvintelor din text si calculul frecventei de aparitie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.783084Z",
     "start_time": "2019-04-11T05:41:30.772513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n",
      "[('be', 0), ('but', 1), ('doth', 2), ('fool', 3), ('he', 4), ('himself', 5), ('is', 6), ('knows', 7), ('man', 8), ('the', 9), ('think', 10), ('to', 11), ('wise', 12)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "print(vect.vocabulary_) # cuvintele distincte din texte\n",
    "\n",
    "# vocabular sortat in ordine crescatoare\n",
    "print(sorted(vect.vocabulary_.items(), key = lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtinerea unui vector bag of words se obtine prin aplicarea metodei `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.793059Z",
     "start_time": "2019-04-11T05:41:30.789069Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = vect.transform(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:30.808018Z",
     "start_time": "2019-04-11T05:41:30.796051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprezentarea ca vectori rari\n",
      ":\b  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "Reprezentarea ca vectori:\n",
      " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Reprezentarea ca vectori rari\\n:\\b{bow}')\n",
    "print('Reprezentarea ca vectori:\\n', bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformarea celor doua subseturi de date in BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:42.845917Z",
     "start_time": "2019-04-11T05:41:30.810040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(text_train)\n",
    "print(repr(X_train))\n",
    "X_test = vect.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:42.850892Z",
     "start_time": "2019-04-11T05:41:42.846904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular: 74849\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensiune vocabular: {len(vect.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:42.937746Z",
     "start_time": "2019-04-11T05:41:42.855882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiunea vocabularului: 74849\n"
     ]
    }
   ],
   "source": [
    "#obtinerea vocabularului\n",
    "feature_names = vect.get_feature_names()\n",
    "print('Dimensiunea vocabularului:', len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:42.948293Z",
     "start_time": "2019-04-11T05:41:42.943729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:42.959266Z",
     "start_time": "2019-04-11T05:41:42.951285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['är', 'ääliöt', 'äänekoski', 'åge', 'åmål', 'æsthetic', 'écran', 'élan', 'émigré', 'émigrés', 'était', 'état', 'étc', 'évery', 'êxtase', 'ís', 'ísnt', 'østbye', 'über', 'üvegtigris']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:42.970238Z",
     "start_time": "2019-04-11T05:41:42.965251Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draper', 'draperies', 'drapery', 'drapes', 'draskovic', 'drastic', 'drastically', 'drat', 'dratch', 'dratic', 'dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[20000:20020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antrenarea si evaluarea unui model de logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:41:44.358773Z",
     "start_time": "2019-04-11T05:41:42.982203Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:42:19.875358Z",
     "start_time": "2019-04-11T05:41:44.362682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea medie de clasificare: 0.8819600000000001\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv = 5, n_jobs=4)\n",
    "print('Acuratetea medie de clasificare: {0}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicare de cross validation pentru parametrul de regularizare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista un parametru de regularizare care determina penalizarea $L_2$ a ponderilor modelului. Efectuam cautarea valorii potrivite a acestui hiperparametru, al carui nume de argument este `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:42:19.880377Z",
     "start_time": "2019-04-11T05:42:19.877380Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:45:18.201567Z",
     "start_time": "2019-04-11T05:42:19.883339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.88816\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:45:18.233482Z",
     "start_time": "2019-04-11T05:45:18.203563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87884\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificarea parametrilor pentru obtinerea BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putem cere retinerea doar a acelor cuvinte care apar intr-un numar minim de documente, de ex 5. In acest fel speram ca se elimina cuvintele care nu sunt larg partajate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:45:28.002361Z",
     "start_time": "2019-04-11T05:45:18.235477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular:  27271\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5)\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "print('Dimensiune vocabular: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:47:03.804053Z",
     "start_time": "2019-04-11T05:45:28.004387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.88744\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:47:03.832974Z",
     "start_time": "2019-04-11T05:47:03.806046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8778\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminare cuvinte neinformative (stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista cuvinte care nu poarta multa informatie: prepozitii, conjunctii, sau cuvinte care se repeta des in toate documentele. Exista doua posibilitati de eliminare a acestora:\n",
    "1. Se foloseste o lista predefinita de stopwords, specifica limbii respective\n",
    "1. Se estimeaza valoarea informationala a cuvintelor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru limba engleza sunt definite stopwords in sklearn.feature_extraction.text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:47:03.839956Z",
     "start_time": "2019-04-11T05:47:03.834969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(len(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:47:03.848932Z",
     "start_time": "2019-04-11T05:47:03.842950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hereafter', 'me', 'call', 'bottom', 'get', 'once', 'thick', 'by', 'give', 'latterly', 'meanwhile', 'at', 'due', 'empty', 'themselves', 'do', 'thereafter', 'inc', 'sincere', 'so']\n"
     ]
    }
   ],
   "source": [
    "print(list(ENGLISH_STOP_WORDS)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:47:12.352194Z",
     "start_time": "2019-04-11T05:47:03.850926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular:  26966\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words='english')\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "print('Dimensiune vocabular: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:47:41.839676Z",
     "start_time": "2019-04-11T05:47:12.354191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.88372\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:47:41.862616Z",
     "start_time": "2019-04-11T05:47:41.841672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87256\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculul valorii tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency - inverse document frequency da pondere unui cuvant care apare frecvent intr-un document, dar nu in multe documente - deci are putere descriptiva pentru documentul in care apare frecvent. Pentru un document $d$ si un cuvant $w$, valoarea tfidf se calculeaza ca:\n",
    "$$\n",
    "tfidf(w, d) = tf(w, d) \\cdot \\log\\left( \\frac{N+1}{N_w+1} \\right) + 1\n",
    "$$\n",
    "unde: $tf(w, d)$ este numarul de aparitii ale lui $w$ in document $d$, $N$ e numarul total de documente din corpus, $N_w$ este numarul de documente din corpus care contin cuvantul $w$. Pentru fiecare document $d$ se calculeaza un astfel de vector, care in final este normalizat in norma $L_2$ la 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:50:12.730197Z",
     "start_time": "2019-04-11T05:47:41.864610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.89424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None), \n",
    "                     LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "param_grid = {'logisticregression__C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(text_train, y_train)\n",
    "print('Best cross validation score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:50:17.381792Z",
     "start_time": "2019-04-11T05:50:12.732192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88508"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizarea de n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW pierde succesiunea cuvintelor. Se pot considera toate succesiunile de n cuvinte - n-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:50:17.388743Z",
     "start_time": "2019-04-11T05:50:17.383754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The fool doth think he is wise,', 'but the wise man knows himself to be a fool']\n"
     ]
    }
   ],
   "source": [
    "print(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:50:17.401706Z",
     "start_time": "2019-04-11T05:50:17.391733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be fool',\n",
       " 'but the',\n",
       " 'doth think',\n",
       " 'fool doth',\n",
       " 'he is',\n",
       " 'himself to',\n",
       " 'is wise',\n",
       " 'knows himself',\n",
       " 'man knows',\n",
       " 'the fool',\n",
       " 'the wise',\n",
       " 'think he',\n",
       " 'to be',\n",
       " 'wise man']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:50:17.417663Z",
     "start_time": "2019-04-11T05:50:17.403702Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T06:24:06.068801Z",
     "start_time": "2019-04-11T05:50:17.419660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100], 'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T06:24:06.167716Z",
     "start_time": "2019-04-11T06:24:06.115068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.91\n",
      "Best parameters:\n",
      "{'logisticregression__C': 100, 'tfidfvectorizer__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T06:24:29.429710Z",
     "start_time": "2019-04-11T06:24:06.168717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90204"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alte modalitati de procesare a textelor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2Vec: [King - Man + Woman = Queen: The Marvelous Mathematics of Computational Linguistics](https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/); [A Beginner's Guide to Word2Vec and Neural Word Embeddings](https://skymind.ai/wiki/word2vec); [Understanding Word Embeddings](https://hackernoon.com/understanding-word-embeddings-a9ff830403ce)\n",
    "* [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
