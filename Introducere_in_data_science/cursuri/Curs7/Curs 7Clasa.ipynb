{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curs 7: Preprocesarea datelor si invatare nesupervizata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invatarea nesupervizata a datelor trateaza cazul in care datele nu sunt etichetate sau nu au vreo alta indicatie - fie ea de natura continua sau discreta - asociata. Orice problema de clasificare sau de regresie se poate transforma intr-o problema de invatare de tip nesupervizat, prin inlaturarea etichetei aferente fiecarei inregistrari. \n",
    "\n",
    "Discutam in acest curs doua tipuri de invatare nesupervizata: \n",
    "* transformare nesupervizata a datelor\n",
    "* clustering\n",
    "\n",
    "Aceste operatii se folosesc frecvent in etapa de explorare a datelor, de exemplu pentru a capata rapid o idee despre structura datelor. In alte cazuri se aplica pe post de metode de preprocesare, de exemplu pentru a aduce valorile de pe dimensiuni diferite la aceleasi scale sau pentru a micsora numarul de date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Transformarea nesupervizata a datelor\n",
    "\n",
    "Transformarea nesupervizata a datelor vizeaza obtinerea unei noi reprezentari a setului initial cu scopul de a le face mai suor de inteles de oameni sau mai utile pentru un algoritm de ML. De exemplu, reducerea de la un numar mare de dimensiuni la 2 sau 3 dimensiuni permite reprezentarea grafica si obtinerea rapida a unei vederi initiale bune asupra datelor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. Scalarea datelor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anumiti algoritmi, precum cei bazati pe calcul de distante sau cei ce lucreaza cu stochastic gradient descent sunt senzitivi la scala datelor: ei prefera ca datele sa fie cu acelasi odin de marime. De exemplu, pentru cazul in care pentru doi vectori $n$-dimensionali $\\mathbf{x} = (x_1, \\dots, x_n)$ respectiv $\\mathbf{y} = (y_1, \\dots, y_n)$ se calculeaza distanta dintre ei cu metrica Euclidiana:\n",
    "$$\n",
    "d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum\\limits_{i=1}^n (x_i - y_i)^2 }\n",
    "$$\n",
    "daca pentru primul indice (prima dimensiune) valorile sunt de ordinul sutelor iar pentru restul dimensiunilor valorile sunt de ordinul zecilor de unitati, atunci valoarea distantei este practic determinata doar de diferenta intre prima dimensiune a fiecarui vector; celelalte dimensiuni nu au nicio influenta.  \n",
    "\n",
    "Exista urmatoarele metode populare de scalare:\n",
    "1. scalarea min-max: toate trasaturile (dimensiunile) sunt transformate in mod independent, astfel incat valorile minime si maxime pe respectiva trasatura sa fie intre un minim si un maxim date. Implementarea e simpla, se calculeaza pentru fiecare dimensiune minimul si maximul, apoi diferenta dintre fiecare valoare si minimul seriei sale este imartita la diferenta intre maximul si minimul seriei din care face parte:\n",
    "1. standardizarea: fiecare dimensiune e astfel transformata incat sa aiba media zero si deviatia standard 1; aceasta se obtine prin: se calculeaza media si deviatia standard pentru fiecare dimensiunne; fiecare serie (dimensiune) se transforma prin impartirea diferentei dintre valorile din seria originara si media seriei la deviatia standard; \n",
    "1. scalarea robusta: ca la punctul anterior, dar se folosesc mediana si quartile ale datelor din fiecare serie, independent;\n",
    "1. normalizarea: se imparte orice vector (presupus nenul) la norma sa. Norma se algee convenabil. In urma transformarii, orice vector va avea norma 1 si se va gasi pe hipersfera de raza 1 centarta in origine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cancer = ...\n",
    "data_names = ...\n",
    "print(data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creare dataframe\n",
    "df_cancer = ...\n",
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriere succinta\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa discrepantele majore intre valorile minime si maxime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer.describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprezentare grafica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'mean concavity'\n",
    "feature2 = 'worst perimeter'\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.axis('equal')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "plt.scatter(df_cancer[feature1], df_cancer[feature2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style=\"ticks\")\n",
    "# sns.pairplot(df_cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cazul in care setul de date esta impartit in set de antrenare si set de validare sau de testare, parametrii folositi pentru transformarea datelor trebuie sa fie retinuti si reutilizati pentru a face aceleasi transformari pe setul exterior celui de antrenare. Este gresit ca seturile de testare sau de validare sa fie transformate cu alte valori, pentru ca modelul (de clasificare/regresie/clustering) determinat pe setul de antrenare are sanse reale sa nu functioneze deloc bine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de aplicare a transformarilor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etichetele y_* sunt utile pentru a demonstra utilitatea scalarii\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = ...\n",
    "#se observa ca datele din X_train nu sunt modificare\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dar obiectul de scalare castiga in starea lui valorile minime si maxime pe fiecare trasatura:\n",
    "print(min_max_scaler.data_min_ == np.min(X_train, axis=0))\n",
    "print(min_max_scaler.data_max_ == np.max(X_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = ...\n",
    "print(np.min(X_train_scaled, axis=0), '\\n', np.max(X_train_scaled, axis=0), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frecvent se cere atat determinarea parametrilor de transfromare, cat si aplicarea transformarii pe un acelsi set de date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformarea setului de testare se face folosind acelasi obiect de scalare obtinut (fitted) pe setul de antrenare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca setul de testare face parte din aceeasi distributie ca si cel de antrenare, ar trebui ca valorile minima si maxime obtinute pe setul de testare sa fie aproximativ 0 si 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minim pe setul de testare, pe fiecare trasatura\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxim pe setul de testare, pe fiecare trasatura\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplul de mai sus se aplica cu mimime modificari altor metode de scalare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_std = standard_scaler.fit_transform(X_train)\n",
    "print('valori medii: ', np.mean(X_train_std, axis = 0))\n",
    "print('deviatie standard: ', np.std(X_train_std, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectul aplicarii unei astfel de preprocesari este dat mai jos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varianta cu date nescalate\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scaled = KNeighborsClassifier(n_neighbors=3)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desigur, putem constata si efectul pe datele standardizate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_std = KNeighborsClassifier(n_neighbors=3)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. Reducerea dimensionalitatii\n",
    "\n",
    "Frecvent, datele disponibile au un numar mare de dimeniuni. In destule cazuri se poate renunta la unele din ele, fara a peirde foarte multa informatie. In plus, se castiga in viteza de calcul, deoarece se ajunge sa se lucreze cu mai putine trasaturi. In destule situatii se poate ajunge la doua trasaturi numerice care pot fi reprezentate in plan, dand posibilitatea unei explorari initiale. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cea mai populara transformare este analiza componentelor principale (Principal Component Analysis, PCA) care se obtine prin metode algebrice relativ simple. \n",
    "\n",
    "Bibliografie recomandata pentru prezentare matematica:\n",
    "1. [A tutorial on Principal Components Analysis](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)\n",
    "1. [A Tutorial on Principal Component Analysis](https://arxiv.org/abs/1404.1100)\n",
    "1. [PCA Whitening](http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sursa: Introduction to Machine Learning with Python, chapter 03\n",
    "\n",
    "fig, axes = plt.subplots(15, 2, figsize=(10, 20))\n",
    "malignant = data_cancer.data[data_cancer.target == 0]\n",
    "benign = data_cancer.data[data_cancer.target == 1]\n",
    "ax = axes.ravel()\n",
    "for i in range(30):\n",
    "    _, bins = np.histogram(data_cancer.data[:, i], bins=50)\n",
    "    ax[i].hist(malignant[:, i], bins=bins, color='red', alpha=.5)\n",
    "    ax[i].hist(benign[:, i], bins=bins, color='green', alpha=.5)\n",
    "    ax[i].set_title(data_cancer.feature_names[i])\n",
    "    ax[i].set_yticks(())\n",
    "ax[0].set_xlabel(\"Feature magnitude\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In histogramele anterioare se observa ca diferite trasaturi individuale au o putere discriminativa mai mica sau mai mare. Ne intereseaza sa consideram doua tarsaturi (nu neaparat din cele originare, pot fi si combinatii liniare ale acestora) astfel incat separarea intre malign si benign sa fie mai buna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cancer, y_cancer = data_cancer.data, data_cancer.target\n",
    "\n",
    "#se aplica in prealabil o scalare a datelor de intrare\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicarea PCA\n",
    "from sklearn.decomposition import PCA\n",
    "...\n",
    "# se tiparesc formele matricelor\n",
    "print(X_scaled.shape)\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_malign = X_pca[y_cancer == 0]\n",
    "X_pca_benign = X_pca[y_cancer == 1]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(X_pca_malign[:, 0], X_pca_malign[:, 1], c='r', marker='^')\n",
    "plt.scatter(X_pca_benign[:, 0], X_pca_benign[:, 1], c='g', marker='o')\n",
    "plt.xlabel('PCA feature 1')\n",
    "plt.ylabel('PCA feature 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasaturile determinate de PCA sunt obtinute pe baza unor transformari liniare ale trasaturilor din setul originar. Se pot afisa coeficientii transformarii liniare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coeficientii (components_) pentru PCA feature 1, respectiv PCA feature 2:', pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O alta metoda destul de populara pentru extragerea de trasaturi este t-SNE. O excelenta prezentare a unuia din autorii algoritmului, Laurens van der Maaten, este [aici](https://www.youtube.com/watch?v=RJVL80Gg3lA). Articolele care prezinta variante ale algoritmului sunt [pe siteul autorului](https://lvdmaaten.github.io/tsne/). Exemplificarea se face pe setul de date `digits` din sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, subplot_kw = {'xticks':(), 'yticks':()})\n",
    "for ax, img, img_cls in zip(axes.ravel(), digits.images, digits.target_names):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title('Cifra ' + str(img_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sursa: https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb\n",
    "\n",
    "# build a PCA model\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(digits.data)\n",
    "# transform the digits data onto the first two principal components\n",
    "digits_pca = pca.transform(digits.data)\n",
    "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
    "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n",
    "plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prin t-SNE se obtin trasaturi mult mai bine diferentiate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sursa: https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=42)\n",
    "# use fit_transform instead of fit, as TSNE has no transform method\n",
    "digits_tsne = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sursa: https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.ylabel(\"t-SNE feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Clustering-ul\n",
    "Clusteringul vizeaza obtinerea de partitii ale setului initial de date. Intre elementele care apartin aceluiasi cluster se considera ca exista relatii de similaritate mai mari decat intre elemente care apartin unor clustere diferite. De exemplu, se doreste impartirea unor imagini cu oameni, in grupuri cu similaritate interna; nu se cunoaste nimic despre indentitatea persoanelor din poze sau metadate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 K-means\n",
    "K-means este cel mai popular algoritm de clustering. El incearca sa grupeze datele in k clustere. Fiecare cluster este definit printr-un centru de greutate (centroid), ale carui coordonate sunt mediile aritmetice ale coordonatelor punctelor care apartin de acelasi cluster. Un punct din setul de instruire sau de testare este asociat cu cel mai apropiat centroid. \n",
    "\n",
    "Bibliografie: \n",
    "1. [K-means, Stanford CS 221](http://stanford.edu/~cpiech/cs221/handouts/kmeans.html)\n",
    "1. [K-means and Hierarchical Clustering, Tutorial Slides by Andrew Moore](https://www.autonlab.org/tutorials/kmeans.html)\n",
    "1. [Curs Sisteme computationale inteligente](https://github.com/lmsasu/cursuri/blob/master/SistemeComputationaleInteligente/SistemeComputationaleInteligente.pdf) sectiunea 8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideea de baza este de a determina prin pasi succesivi o pozitionare a centroizilor, precum si o impartire a setului initial de instruire in subseturi (posibil, desi arareori, vide) asociate fiecarui centroid. \n",
    "\n",
    "![k-means](./images/kmeans.png)\n",
    "Sursa: ref [1] de mai sus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "\n",
    "#aplicare algoritm kmeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "centroids = ...\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='^', c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urmatoarele situatii sunt defavorabile pentru algoritmul k-means:\n",
    "1. Numar de clustere necunoscut apriori\n",
    "1. cazul in care datele nu au forma aproximativ globulara si de diametre egale\n",
    "1. Densitati de distributie diferite in clustere\n",
    "1. Alegere neinspirata a pozitiilor centroizilor; pot rezulta centroizi care partitioneaza un grup de puncte; pot rezulta centroizi orfani = fara puncte asociate\n",
    "1. Nu trateaza bine situatiile in care clusterele nu sunt sferice.\n",
    "\n",
    "Pentru aceasta ultiam situatie dam exemplul de mai jos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reprezentare set de date; culorile sunt optionale\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "#reprezentare clustere\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru situatia in care determinarea apartenentei de clustere este data de denistatea de repartitie, mai degraba decat de distanta (cum e cazul de mai sus), algoritmi precum DBSCAN sunt recomandati. \n",
    "\n",
    "Ideea algoritmului DBSCAN este de a determian clustere ce acopera regiuni dense de date; clusterele sunt separate de regiuni cu densitate mica. DBSCAN nu necesita precizarea apriori a numarului de clustere (cu toate ca are alti hiperparametri ce trebuie specificati), poate eticheta unele date ca fiind zgomot, adica neafiliate niciunui cluster. \n",
    "\n",
    "Bibliografie:\n",
    "1. [A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()#valoarea implicita pentru eps e 0.5\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rezulattul de mai sus arata ca se obtine un singur cluster. Prin modificarea valorilor hiperparametrilor eps si min_samples se obtin rezultate complet diferite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
